{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Downstream Model Impact Assessment\n",
    "\n",
    "This experiment evaluates how imputation quality affects downstream prediction models.\n",
    "We compare four imputation methods by training SVR models on imputed datasets and measuring prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from models.linear_regression import LinearRegression\n",
    "from models.gaussian_process import GaussianProcessRegression\n",
    "from imputation.chained_imputer import ChainedImputer\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset with hierarchical structure\n",
    "n_samples = 300\n",
    "n_groups = 4\n",
    "\n",
    "# Create group labels\n",
    "groups = np.random.choice(['A', 'B', 'C', 'D'], size=n_samples)\n",
    "\n",
    "# Group-specific offsets\n",
    "group_effects = {'A': 2.0, 'B': -1.5, 'C': 1.0, 'D': -0.5}\n",
    "\n",
    "# Generate features\n",
    "X1 = np.random.randn(n_samples)\n",
    "X2 = np.random.randn(n_samples)\n",
    "X3 = np.random.randn(n_samples)  # Will be masked\n",
    "X4 = np.random.randn(n_samples)\n",
    "\n",
    "# Generate target with non-linear relationship\n",
    "y = 3.0 * X1 + 2.0 * X2 + 1.5 * X3 + 0.5 * X1**2 + 0.3 * X2 * X3\n",
    "for i, group in enumerate(groups):\n",
    "    y[i] += group_effects[group]\n",
    "y += np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'group': groups,\n",
    "    'x1': X1,\n",
    "    'x2': X2,\n",
    "    'x3': X3,\n",
    "    'x4': X4,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split Data and Mask Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "\n",
    "# Mask 30% of x3 values in training data only\n",
    "mask_ratio = 0.3\n",
    "n_mask = int(len(train_data) * mask_ratio)\n",
    "mask_indices = np.random.choice(train_data.index, size=n_mask, replace=False)\n",
    "\n",
    "train_data_masked = train_data.copy()\n",
    "train_data_masked.loc[mask_indices, 'x3'] = np.nan\n",
    "\n",
    "print(f\"\\nMasked {n_mask} values in training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform Imputation with Four Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define imputation methods\n",
    "methods = {\n",
    "    'Ordinary LR': ChainedImputer(\n",
    "        base_model=LinearRegression(),\n",
    "        random_effects=None,\n",
    "        n_imputations=1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Hierarchical LR': ChainedImputer(\n",
    "        base_model=LinearRegression(),\n",
    "        random_effects=['group'],\n",
    "        n_imputations=1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Ordinary GP': ChainedImputer(\n",
    "        base_model=GaussianProcessRegression(),\n",
    "        random_effects=None,\n",
    "        n_imputations=1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Hierarchical GP': ChainedImputer(\n",
    "        base_model=GaussianProcessRegression(),\n",
    "        random_effects=['group'],\n",
    "        n_imputations=1,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Perform imputation\n",
    "imputed_datasets = {}\n",
    "for method_name, imputer in methods.items():\n",
    "    print(f\"Imputing with {method_name}...\")\n",
    "    imputed = imputer.fit_transform(train_data_masked)\n",
    "    imputed_datasets[method_name] = imputed[0]\n",
    "\n",
    "print(\"\\nImputation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train SVR Models on Imputed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SVR hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "    'epsilon': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Feature columns (exclude group and target)\n",
    "feature_cols = ['x1', 'x2', 'x3', 'x4']\n",
    "\n",
    "results = []\n",
    "\n",
    "for method_name, train_imputed in imputed_datasets.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training SVR with {method_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_imputed[feature_cols].values\n",
    "    y_train = train_imputed['y'].values\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_data[feature_cols].values\n",
    "    y_test = test_data['y'].values\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    svr = SVR(kernel='rbf')\n",
    "    grid_search = GridSearchCV(\n",
    "        svr, param_grid, cv=5, \n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1, verbose=0\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_svr = grid_search.best_estimator_\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = best_svr.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Test MSE: {mse:.4f}\")\n",
    "    print(f\"Test R²: {r2:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'Method': method_name,\n",
    "        'MSE': mse,\n",
    "        'R2': r2,\n",
    "        'Best_Params': grid_search.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOWNSTREAM MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results_df[['Method', 'MSE', 'R2']].to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Best method (lowest MSE): {results_df.loc[results_df['MSE'].idxmin(), 'Method']}\")\n",
    "print(f\"Best method (highest R²): {results_df.loc[results_df['R2'].idxmax(), 'Method']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MSE comparison\n",
    "ax1 = axes[0]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "ax1.bar(results_df['Method'], results_df['MSE'], color=colors)\n",
    "ax1.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "ax1.set_title('SVR Performance: MSE Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar(results_df['Method'], results_df['R2'], color=colors)\n",
    "ax2.set_ylabel('R² Score', fontsize=12)\n",
    "ax2.set_title('SVR Performance: R² Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiment_2_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'experiment_2_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This experiment demonstrates how imputation quality affects downstream prediction models. Better imputation methods should lead to better SVR performance, as measured by lower MSE and higher R² scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
